{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83c41c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous character: A\n",
      "Previous character: A\n",
      "Previous character: B\n",
      "Previous character: B\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = None  # Variable to store the previous character\n",
    "c_printed = False\n",
    "\n",
    "while True:\n",
    "\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # image to draw\n",
    "                hand_landmarks,  # model output\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "        \n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            print(\"Previous character:\", previous_character)\n",
    "            c_printed = True\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "        #previous_character = predicted_character  # Update the previous character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d949443b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m list_ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m list_ :\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(i)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "list_ = [\"k\" , \"a\" , \"r\" , \"i\" , \"m\"]\n",
    "for i in list_ :\n",
    "    text.add(i)\n",
    "    print(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b091af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karim\n"
     ]
    }
   ],
   "source": [
    "list_ = [\"k\", \"a\", \"r\", \"i\", \"m\"]\n",
    "text = \"\".join(list_)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138d1755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized word: AABBABA\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = None\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Recognized word:\", recognized_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317f3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = None\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((200, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(100)  # Display each character for 1 second\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1dd379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aed75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fb8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            # Speak the recognized word\n",
    "            tts = gTTS(text=recognized_word, lang='en')\n",
    "            tts.save(\"recognized_word.mp3\")\n",
    "            os.system(\"mpg321 recognized_word.mp3\")\n",
    "\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((100, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(1000)  # Display each character for 1 second\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4168d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de4598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee68a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            # Speak the recognized word in real-time\n",
    "            engine.say(recognized_word)\n",
    "            engine.runAndWait()\n",
    "\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((100, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(100)  # Display each character for 1 second\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a7d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b30802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c21ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = None\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a GUI window with a button\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech\")\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word)\n",
    "speak_button.pack()\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            # Speak the recognized word in real-time\n",
    "            engine.say(recognized_word)\n",
    "            engine.runAndWait()\n",
    "\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((100, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(100)  # Display each character for 1 second\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4fa8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8054e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = None\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a GUI window with a button\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech\")\n",
    "\n",
    "def show_speak_button():\n",
    "    speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word)\n",
    "    speak_button.pack()\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            # Show the speak button when letter \"C\" is recognized\n",
    "            show_speak_button()\n",
    "\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((100, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(1000)  # Display each character for 1 second\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de293c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be46f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "is_button_visible = False  # Flag to track if the button is visible\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a GUI window with a button\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech\")\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word)\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            if not is_button_visible:\n",
    "                # Show the speak button only once\n",
    "                is_button_visible = True\n",
    "                speak_button.pack()\n",
    "\n",
    "            # Calculate the coordinates of the rectangle\n",
    "            x1 = int(min(x_) * W) - 10\n",
    "            y1 = int(min(y_) * H) - 10\n",
    "\n",
    "            x2 = int(max(x_) * W) - 10\n",
    "            y2 = int(max(y_) * H) - 10\n",
    "\n",
    "            # Draw the rectangle\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((100, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(100)  # Display each character for 1 second\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec750e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABABA\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "    speak_button.pack_forget()  # Hide the button after speaking\n",
    "\n",
    "# Create a GUI window with a button\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech\")\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word)\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            # Show the speak button for the current character\n",
    "            if previous_character is not None:\n",
    "                speak_button.pack()  # Show the button\n",
    "\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((100, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(300)  # Display each character for 1 second\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "print(recognized_word)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64598db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABAB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "    speak_button.pack_forget()  # Hide the button after speaking\n",
    "\n",
    "# Create a GUI window with a button\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech\")\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word)\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            # Show the speak button for the current character\n",
    "            if previous_character is not None:\n",
    "                speak_button.pack()  # Show the button\n",
    "\n",
    "            cv2.imshow('Playback', cv2.putText(np.zeros((100, 500, 3), np.uint8), recognized_word, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3, cv2.LINE_AA))\n",
    "            cv2.waitKey(300)  # Display each character for 0.3 seconds\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "print(recognized_word)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595180e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee93403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABBA\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a GUI window to display characters and the speak button\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech\")\n",
    "\n",
    "# Create a label to display the recognized characters\n",
    "recognized_label = tk.Label(root, text=\"\", font=(\"Helvetica\", 36))\n",
    "recognized_label.pack()\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word)\n",
    "speak_button.pack()\n",
    "speak_button.config(state=\"disabled\")  # Disable the button initially\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "            speak_button.config(state=\"normal\")  # Enable the button\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "        # Update the recognized characters label\n",
    "        recognized_label.config(text=recognized_word)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord(\"k\"):\n",
    "        break\n",
    "\n",
    "print(recognized_word)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3a6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3e04e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER_PC_SA\\anaconda1\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\USER_PC_SA\\anaconda1\\lib\\tkinter\\__init__.py\", line 814, in callit\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\USER_PC_SA\\AppData\\Local\\Temp\\ipykernel_20832\\3326978205.py\", line 59, in update_video_feed\n",
      "    H, W, _ = frame.shape\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack()\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20))\n",
    "speak_button.pack()\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack()\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            recognized_label.config(text=recognized_word)\n",
    "\n",
    "            cv2.waitKey(300)  # Display each character for 0.3 seconds\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(10, update_video_feed)\n",
    "    \n",
    "\n",
    "\n",
    "update_video_feed()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887165ab",
   "metadata": {},
   "source": [
    "## good virsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981d41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack()\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20))\n",
    "speak_button.pack()\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack()\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # Video capture failed, wait and try again\n",
    "        canvas.after(10, update_video_feed)\n",
    "        return\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"C\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            recognized_label.config(text=recognized_word)\n",
    "\n",
    "            cv2.waitKey(300)  # Display each character for 0.3 seconds\n",
    "        elif predicted_character != \"C\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(10, update_video_feed)\n",
    "\n",
    "update_video_feed()\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa69c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357e5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: ' ', 1: 'A', 2: 'B' , 3: 'C' , 4: 'D' , 5: 'E' , 6: 'F' , 7: ' G' , 8: 'H' , 9: 'I'  ,10: 'K' , 11 : '' }\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a function to delete the last character\n",
    "def delete_last_character():\n",
    "    global recognized_word\n",
    "    if len(recognized_word) > 0:\n",
    "        recognized_word = recognized_word[:-1]  # Remove the last character\n",
    "        recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack()\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20))\n",
    "speak_button.pack()\n",
    "\n",
    "# Create a button to delete the last character\n",
    "delete_button = tk.Button(root, text=\"Delete Last Character\", command=delete_last_character, font=(\"Helvetica\", 20))\n",
    "delete_button.pack()\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack()\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # Video capture failed, wait and try again\n",
    "        canvas.after(10, update_video_feed)\n",
    "        return\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            recognized_label.config(text=recognized_word)\n",
    "\n",
    "            cv2.waitKey(500)  # Display each character for 0.4 seconds\n",
    "        elif predicted_character != \"\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(10, update_video_feed)\n",
    "\n",
    "update_video_feed()\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98682143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: ' ', 1: 'A', 2: 'B' , 3: 'C' , 4: 'D' , 5: 'E' , 6: 'F' , 7: ' G' , 8: 'H' , 9: 'I'  ,10: 'K' , 11 : '' }\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a function to delete the last character\n",
    "def delete_last_character():\n",
    "    global recognized_word\n",
    "    if len(recognized_word) > 0:\n",
    "        recognized_word = recognized_word[:-1]  # Remove the last character\n",
    "        recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack()\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20))\n",
    "speak_button.pack()\n",
    "\n",
    "# Create a button to delete the last character\n",
    "delete_button = tk.Button(root, text=\"Delete Last Character\", command=delete_last_character, font=(\"Helvetica\", 20))\n",
    "delete_button.pack()\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack()\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # Video capture failed, wait and try again\n",
    "        canvas.after(200 , update_video_feed)\n",
    "        \n",
    "        return\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            recognized_label.config(text=recognized_word)\n",
    "\n",
    "            cv2.waitKey(500)  # Display each character for 0.4 seconds\n",
    "        elif predicted_character != \"\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(200 , update_video_feed)\n",
    "    \n",
    "update_video_feed()\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79865403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "from googletrans import Translator  # Import Translator from googletrans library\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: ' ', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'K', 11: ''}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a function to delete the last character\n",
    "def delete_last_character():\n",
    "    global recognized_word\n",
    "    if len(recognized_word) > 0:\n",
    "        recognized_word = recognized_word[:-1]  # Remove the last character\n",
    "        recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a function to translate text to Arabic\n",
    "def translate_to_arabic():\n",
    "    global recognized_word\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(recognized_word, src='en', dest='ar')\n",
    "    recognized_word = translated_text.text\n",
    "    recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack(side=\"top\")  \n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20))\n",
    "speak_button.pack()\n",
    "\n",
    "# Create a button to delete the last character\n",
    "delete_button = tk.Button(root, text=\"Delete Last Character\", command=delete_last_character, font=(\"Helvetica\", 20))\n",
    "delete_button.pack()\n",
    "\n",
    "# Create a button to translate to Arabic\n",
    "translate_button = tk.Button(root, text=\"Translate to Arabic\", command=translate_to_arabic, font=(\"Helvetica\", 20))\n",
    "translate_button.pack()\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=740, height=580)\n",
    "canvas.pack()\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # Video capture failed, wait and try again\n",
    "        canvas.after(200, update_video_feed)\n",
    "\n",
    "        return\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            recognized_label.config(text=recognized_word)\n",
    "\n",
    "            cv2.waitKey(500)  # Display each character for 0.4 seconds\n",
    "        elif predicted_character != \"\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(200, update_video_feed)\n",
    "\n",
    "update_video_feed()\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003d499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "from googletrans import Translator\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: ' ', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'K', 11: ''}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a function to delete the last character\n",
    "def delete_last_character():\n",
    "    global recognized_word\n",
    "    if len(recognized_word) > 0:\n",
    "        recognized_word = recognized_word[:-1]  # Remove the last character\n",
    "        recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a function to translate text to Arabic\n",
    "def translate_to_arabic():\n",
    "    global recognized_word\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(recognized_word, src='en', dest='ar')\n",
    "    recognized_word = translated_text.text\n",
    "    recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack(side=\"top\")  # Place the canvas at the top\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack(side=\"top\")  # Place the label below the canvas\n",
    "\n",
    "# Create a button to translate to Arabic\n",
    "translate_button = tk.Button(root, text=\"Translate to Arabic\", command=translate_to_arabic, font=(\"Helvetica\", 20) , bg=\"red\")\n",
    "translate_button.pack(side=\"left\")  # Place the button below the canvas on the left\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20) , bg=\"yellow\")\n",
    "speak_button.pack(side=\"left\")  # Place the button below the canvas in the center\n",
    "\n",
    "# Create a button to delete the last character\n",
    "delete_button = tk.Button(root, text=\"Delete Last Character\", command=delete_last_character, font=(\"Helvetica\", 20) , bg=\"green\")\n",
    "delete_button.pack(side=\"left\")  # Place the button below the canvas on the right\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # Video capture failed, wait and try again\n",
    "        canvas.after(200 , update_video_feed)\n",
    "\n",
    "        return\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            recognized_label.config(text=recognized_word)\n",
    "\n",
    "            cv2.waitKey(400)  # Display each character for 0.4 seconds\n",
    "        elif predicted_character != \"\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(400 , update_video_feed)\n",
    "\n",
    "update_video_feed()\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c8625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1db044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "from googletrans import Translator\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: ' ', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'K', 11: ''}\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a function to delete the last character\n",
    "def delete_last_character():\n",
    "    global recognized_word\n",
    "    if len(recognized_word) > 0:\n",
    "        recognized_word = recognized_word[:-1]  # Remove the last character\n",
    "        recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a function to translate text to Arabic\n",
    "def translate_to_arabic():\n",
    "    global recognized_word\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(recognized_word, src='en', dest='ar')\n",
    "    recognized_word = translated_text.text\n",
    "    recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a function to quit the application\n",
    "def quit_application():\n",
    "    root.quit()  # Quit the tkinter application\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    root.destroy()\n",
    "\n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack(side=\"top\")  # Place the canvas at the top\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack(side=\"top\")  # Place the label below the canvas\n",
    "\n",
    "# Create a button to translate to Arabic\n",
    "translate_button = tk.Button(root, text=\"Translate to Arabic\", command=translate_to_arabic, font=(\"Helvetica\", 20), bg=\"red\")\n",
    "translate_button.pack(side=\"left\")  # Place the button below the canvas on the left\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20), bg=\"yellow\")\n",
    "speak_button.pack(side=\"left\")  # Place the button below the canvas in the center\n",
    "\n",
    "# Create a button to delete the last character\n",
    "delete_button = tk.Button(root, text=\"Delete Last Character\", command=delete_last_character, font=(\"Helvetica\", 20), bg=\"green\")\n",
    "delete_button.pack(side=\"left\")  # Place the button below the canvas on the right\n",
    "\n",
    "# Create a \"Quit\" button to close the application\n",
    "quit_button = tk.Button(root, text=\"Quit\", command=quit_application, font=(\"Helvetica\", 20), bg=\"blue\")\n",
    "quit_button.pack(side=\"left\")  # Place the button below the canvas on the right\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # Video capture failed, wait and try again\n",
    "        canvas.after(200 , update_video_feed)\n",
    "\n",
    "        return\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == \"\" and not c_printed:\n",
    "            recognized_word += previous_character\n",
    "            c_printed = True\n",
    "\n",
    "            recognized_label.config(text=recognized_word)\n",
    "\n",
    "            cv2.waitKey(500)  # Display each character for 0.4 seconds\n",
    "        elif predicted_character != \"\":\n",
    "            c_printed = False\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(200 , update_video_feed)\n",
    "\n",
    "update_video_feed()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b245a",
   "metadata": {},
   "source": [
    "<img src=\"https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5308f9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "sign_start_time = 0\n",
    "sign_timeout = 1.25\n",
    "\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {0: ' ', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'K', 11 : 'L',\n",
    "               12 : 'M', 13 : 'N' , 14 : 'O' , 15 : 'p' , 16 : 'Q' , 17 : 'R' , 18 : 'S' , 19 : 'T' , 20 : 'U' , 21 : 'V',\n",
    "               22 : 'W' , 23 : 'X' ,  24: 'Y' , 25 :'' , 26 :'J' , 27 : 'Z'}\n",
    "\n",
    "previous_character = \"\"\n",
    "c_printed = False\n",
    "recognized_word = \"\"  # Variable to store the recognized word\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a function to speak the recognized word\n",
    "def speak_recognized_word():\n",
    "    engine.say(recognized_word)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Create a function to delete the last character\n",
    "def delete_last_character():\n",
    "    global recognized_word\n",
    "    if len(recognized_word) > 0:\n",
    "        recognized_word = recognized_word[:-1]  # Remove the last character\n",
    "        recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a function to translate text to Arabic\n",
    "def translate_to_arabic():\n",
    "    global recognized_word\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(recognized_word, src='en', dest='ar')\n",
    "    recognized_word = translated_text.text\n",
    "    recognized_label.config(text=recognized_word)\n",
    "\n",
    "# Create a function to clear the recognized text\n",
    "def clear_recognized_text():\n",
    "    global recognized_word\n",
    "    recognized_word = \"\"  # Clear the recognized word\n",
    "    recognized_label.config(text=recognized_word)\n",
    "\n",
    "    \n",
    "# Create a function to quit the application\n",
    "def quit_application():\n",
    "    root.quit()  # Quit the tkinter application\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    root.destroy()\n",
    "    \n",
    "# Create a function to save the recognized text to a file\n",
    "def save_to_file():\n",
    "    global recognized_word\n",
    "    if recognized_word:\n",
    "        with open(\"recognized_text.txt\", \"a\") as file:\n",
    "            file.write(recognized_word + \"\\n\")\n",
    "        recognized_word = \"\"  # Clear the recognized word\n",
    "        recognized_label.config(text=recognized_word)\n",
    "    \n",
    "# Create a GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text-to-Speech Recognition\")\n",
    "\n",
    "# Create a canvas to display the video feed\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack(side=\"top\")  # Place the canvas at the top\n",
    "\n",
    "# Create a label for recognized letters\n",
    "recognized_label = tk.Label(root, text=recognized_word, font=(\"Helvetica\", 30))\n",
    "recognized_label.pack(side=\"top\")  # Place the label below the canvas\n",
    "\n",
    "# Create a button to translate to Arabic\n",
    "translate_button = tk.Button(root, text=\"Translate to Arabic\", command=translate_to_arabic, font=(\"Helvetica\", 20), bg=\"red\")\n",
    "translate_button.pack(side=\"left\")  # Place the button below the canvas on the left\n",
    "\n",
    "# Create a button to trigger text-to-speech\n",
    "speak_button = tk.Button(root, text=\"Speak Recognized Word\", command=speak_recognized_word, font=(\"Helvetica\", 20), bg=\"yellow\")\n",
    "speak_button.pack(side=\"left\")  # Place the button below the canvas in the center\n",
    "\n",
    "# Create a button to delete the last character\n",
    "delete_button = tk.Button(root, text=\"Delete Last Character\", command=delete_last_character, font=(\"Helvetica\", 20), bg=\"green\")\n",
    "delete_button.pack(side=\"left\")  # Place the button below the canvas on the right\n",
    "\n",
    "# Create a \"Quit\" button to close the application\n",
    "quit_button = tk.Button(root, text=\"Quit\", command=quit_application, font=(\"Helvetica\", 20), bg=\"blue\")\n",
    "quit_button.pack(side=\"left\")  # Place the button below the canvas on the right\n",
    "\n",
    "clear_button = tk.Button(root, text=\"Clear Text\", command=clear_recognized_text, font=(\"Helvetica\", 20), bg=\"orange\")\n",
    "clear_button.pack(side=\"left\")\n",
    "\n",
    "# Create a button to save the recognized text to a file\n",
    "save_button = tk.Button(root, text=\"Save to File\", command=save_to_file, font=(\"Helvetica\", 20), bg=\"purple\")\n",
    "save_button.pack(side=\"left\")\n",
    "\n",
    "\n",
    "# Declare previous_character as a global variable\n",
    "global previous_character\n",
    "\n",
    "def update_video_feed():\n",
    "    global c_printed, recognized_word, previous_character, sign_start_time, img  # Declare global variables\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # Video capture failed, wait and try again\n",
    "        canvas.after(100 , update_video_feed)\n",
    "\n",
    "        return\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the recognized character next to the previous ones during playback\n",
    "        if predicted_character == previous_character:\n",
    "            if sign_start_time is None:\n",
    "                # If this is the start of the sign, record the start time\n",
    "                sign_start_time = time.time()\n",
    "            else:\n",
    "                current_time = time.time()\n",
    "                if current_time - sign_start_time >= sign_timeout:\n",
    "                    # The sign has been held for 3 seconds, add the character to recognized_word\n",
    "                    recognized_word += predicted_character\n",
    "                    recognized_label.config(text=recognized_word)\n",
    "                    sign_start_time = None  # Reset the start time\n",
    "        else:\n",
    "            sign_start_time = None\n",
    "\n",
    "        previous_character = predicted_character\n",
    "\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "    canvas.img = img\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "    canvas.after(100 , update_video_feed)\n",
    "\n",
    "update_video_feed()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492d1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a1c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
